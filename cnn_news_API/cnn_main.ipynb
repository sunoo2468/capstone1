{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816c53a4",
   "metadata": {},
   "source": [
    "# 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a14c4-156f-442b-bb24-b4af7aa809b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_and_store_news.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1) ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('NEWSAPI_KEY')\n",
    "if not API_KEY:\n",
    "    logger.error(\"NEWSAPI_KEY not found in environment variables\")\n",
    "    raise ValueError(\"NEWSAPI_KEY not found\")\n",
    "\n",
    "# 2) Í∞êÏÑ± Î∂ÑÏÑù & NER Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# 3) ÎÇ†Ïßú Î≤îÏúÑ ÏÑ§Ï†ï\n",
    "today     = datetime.today()\n",
    "from_date = (today - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "to_date   = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# 4) NASDAQ Îß§Ìïë\n",
    "BASE_DIR    = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "nasdaq_csv  = os.path.join(BASE_DIR, 'data', 'nasdaq_screener_1744184912302.csv')\n",
    "ticker_name_map = {}\n",
    "if os.path.exists(nasdaq_csv):\n",
    "    df_nasdaq = pd.read_csv(nasdaq_csv)\n",
    "    required_cols = {\"Symbol\",\"Name\",\"Market Cap\",\"Sector\"}\n",
    "    missing = required_cols - set(df_nasdaq.columns)\n",
    "    if missing:\n",
    "        logger.error(f\"Missing columns in NASDAQ CSV: {missing}\")\n",
    "        raise RuntimeError(f\"NASDAQ CSV missing: {missing}\")\n",
    "    mask = (\n",
    "        (df_nasdaq[\"Market Cap\"] > 0) &\n",
    "        df_nasdaq[\"Sector\"].notnull() &\n",
    "        ~df_nasdaq[\"Name\"].str.contains(\"Units|Rights|Warrant|Preferred|Depositary|Series\", case=False)\n",
    "    )\n",
    "    df_clean = df_nasdaq[mask].drop_duplicates(subset=\"Symbol\")\n",
    "    df_clean[\"Symbol\"] = df_clean[\"Symbol\"].str.lower().str.strip()\n",
    "    df_clean[\"Name\"]   = df_clean[\"Name\"].str.strip()\n",
    "    ticker_name_map    = dict(zip(df_clean[\"Symbol\"], df_clean[\"Name\"]))\n",
    "    logger.info(f\"Loaded {len(ticker_name_map)} ticker-name mappings\")\n",
    "else:\n",
    "    logger.warning(\"NASDAQ mapping CSV not found; continuing without ticker mapping\")\n",
    "\n",
    "# 5) ÏÇ∞ÏóÖ ÌÇ§ÏõåÎìú Î°úÎìú\n",
    "kw_path = os.path.join(BASE_DIR, 'data', 'industry_keywords.txt')\n",
    "with open(kw_path, 'r', encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "mod = ast.parse(contents)\n",
    "industry_keywords = []\n",
    "for node in mod.body:\n",
    "    if isinstance(node, ast.Assign):\n",
    "        for target in node.targets:\n",
    "            if getattr(target, 'id', None) == 'industry_keywords':\n",
    "                industry_keywords = ast.literal_eval(node.value)\n",
    "                break\n",
    "if not industry_keywords:\n",
    "    logger.error(\"No industry_keywords loaded\")\n",
    "    raise RuntimeError(\"industry_keywords.txt parsing failed\")\n",
    "logger.info(f\"Loaded {len(industry_keywords)} industry keywords\")\n",
    "\n",
    "# 6) ÌÅ¨Î°§ÎßÅ ÌååÎùºÎØ∏ÌÑ∞\n",
    "API_URL   = \"https://newsapi.org/v2/everything\"\n",
    "params    = {\n",
    "    'q':        'nasdaq OR stock OR technology OR innovation',\n",
    "    'from':     from_date,\n",
    "    'to':       to_date,\n",
    "    'language': 'en',\n",
    "    'pageSize': 100,\n",
    "    'domains':  'cnn.com',\n",
    "    'apiKey':   API_KEY,\n",
    "    'sortBy':   'publishedAt'\n",
    "}\n",
    "MAX_PAGES = 1  # Î¨¥Î£å ÌîåÎûú ÏµúÎåÄ 100Í±¥Îßå ÏàòÏßë\n",
    "\n",
    "# 7) Îâ¥Ïä§ ÏàòÏßë\n",
    "articles = []\n",
    "for page in range(1, MAX_PAGES + 1):\n",
    "    params['page'] = page\n",
    "    res = requests.get(API_URL, params=params, headers={'User-Agent':'Mozilla/5.0'})\n",
    "    try:\n",
    "        data = res.json()\n",
    "    except ValueError:\n",
    "        logger.error(\"Invalid JSON response from NewsAPI\")\n",
    "        break\n",
    "\n",
    "    if res.status_code != 200:\n",
    "        code = data.get('code','')\n",
    "        if code == 'maximumResultsReached':\n",
    "            logger.warning(\"Reached free-plan limit (100 articles). Stopping.\")\n",
    "        else:\n",
    "            logger.error(f\"API error {res.status_code}: {data.get('message','')}\")\n",
    "        break\n",
    "\n",
    "    articles_batch = data.get('articles')\n",
    "    if not isinstance(articles_batch, list):\n",
    "        logger.error(\"Unexpected API response structure\")\n",
    "        break\n",
    "    if not articles_batch:\n",
    "        break\n",
    "\n",
    "    for art in articles_batch:\n",
    "        full = ' '.join(filter(None, [\n",
    "            art.get('title'),\n",
    "            art.get('description'),\n",
    "            art.get('content')\n",
    "        ]))\n",
    "        articles.append({\n",
    "            'title':        art.get('title',''),\n",
    "            'description':  art.get('description',''),\n",
    "            'content':      art.get('content',''),\n",
    "            'url':          art.get('url',''),\n",
    "            'published_at': art.get('publishedAt',''),\n",
    "            'full_text':    full\n",
    "        })\n",
    "    time.sleep(1)\n",
    "\n",
    "logger.info(f\"Crawled total articles: {len(articles)}\")\n",
    "\n",
    "# 8) DataFrame ÏÉùÏÑ± & Ï≤¥ÌÅ¨\n",
    "df = pd.DataFrame(articles)\n",
    "if df.empty:\n",
    "    logger.error(\"No articles collected; exiting\")\n",
    "    raise RuntimeError(\"No data to process\")\n",
    "\n",
    "# 9) Í∞êÏÑ±¬∑ÌÇ§ÏõåÎìú Ï∂îÏ∂ú Ìï®Ïàò\n",
    "def extract_positive_orgs(text, max_orgs=3):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    if vader.polarity_scores(text)['compound'] <= 0:\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    orgs = [ent.text.lower() for ent in doc.ents if ent.label_=='ORG']\n",
    "    found, added = [], set()\n",
    "    for name in orgs:\n",
    "        for sym, nm in ticker_name_map.items():\n",
    "            if name in nm.lower() or nm.lower() in name:\n",
    "                if sym and sym not in added:\n",
    "                    found.append(sym); added.add(sym)\n",
    "                break\n",
    "        if len(found) >= max_orgs:\n",
    "            break\n",
    "    return found\n",
    "\n",
    "def extract_industry_keywords(text):\n",
    "    txt = text.lower() if isinstance(text, str) else \"\"\n",
    "    return [kw for kw in industry_keywords if kw in txt]\n",
    "\n",
    "# 10) ÌïÑÌÑ∞ÎßÅ & Ï†ïÏ†ú\n",
    "df['positive_orgs']  = df['full_text'].apply(extract_positive_orgs)\n",
    "df['industry_keys']  = df['full_text'].apply(extract_industry_keywords)\n",
    "df_f = df[(df['positive_orgs'].str.len()>0) | (df['industry_keys'].str.len()>0)].copy()\n",
    "\n",
    "for col in ['positive_orgs','industry_keys']:\n",
    "    df_f[col] = (\n",
    "        df_f[col]\n",
    "        .apply(lambda lst: [s for s in lst if s])\n",
    "        .apply(lambda lst: ','.join(lst))\n",
    "    )\n",
    "\n",
    "# 11) SQLite DB Ï†ÄÏû•\n",
    "db_path = os.path.join(BASE_DIR, 'data', 'cnn_news.db')\n",
    "engine  = create_engine(f\"sqlite:///{db_path}\")\n",
    "df_f.to_sql('cnn_positive_news', engine, if_exists='replace', index=False)\n",
    "logger.info(f\"Stored {len(df_f)} positive articles in `cnn_positive_news` table\")\n",
    "\n",
    "# 12) ÌÜµÍ≥Ñ Ï∂úÎ†•\n",
    "org_counts      = Counter(df_f['positive_orgs'].str.split(',').sum())\n",
    "industry_counts = Counter(df_f['industry_keys'].str.split(',').sum())\n",
    "org_counts.pop('', None)\n",
    "industry_counts.pop('', None)\n",
    "\n",
    "logger.info(f\"Top Í∏∞ÏóÖ ÌÇ§ÏõåÎìú: {org_counts.most_common(10)}\")\n",
    "logger.info(f\"Top ÏÇ∞ÏóÖ ÌÇ§ÏõåÎìú: {industry_counts.most_common(10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775227b3",
   "metadata": {},
   "source": [
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c2eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_industry_emb.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import spacy\n",
    "import logging\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1) Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "BASE_DIR   = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "GLOVE_PATH = os.path.join(BASE_DIR, 'data', 'glove.6B.300d.txt')\n",
    "KW_PATH    = os.path.join(BASE_DIR, 'data', 'industry_keywords.txt')\n",
    "EMBED_OUT  = os.path.join(BASE_DIR, 'data', 'industry_emb_32d.npy')\n",
    "VOCAB_OUT  = os.path.join(BASE_DIR, 'data', 'industry_vocab.json')\n",
    "\n",
    "# 2) spaCy Î™®Îç∏ Î°úÎìú (phrase fallback Ïö©)\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# 3) ÏÇ∞ÏóÖ ÌÇ§ÏõåÎìú Î°úÎìú\n",
    "with open(KW_PATH, 'r', encoding='utf-8') as f:\n",
    "    src = f.read()\n",
    "mod = ast.parse(src)\n",
    "industry_keywords = []\n",
    "for node in mod.body:\n",
    "    if isinstance(node, ast.Assign):\n",
    "        for targ in node.targets:\n",
    "            if getattr(targ, 'id', None) == 'industry_keywords':\n",
    "                industry_keywords = ast.literal_eval(node.value)\n",
    "                break\n",
    "\n",
    "if not industry_keywords:\n",
    "    logger.error(\"Failed to load any industry_keywords\")\n",
    "    raise RuntimeError(\"industry_keywords.txt parsing failed\")\n",
    "logger.info(f\"Loaded {len(industry_keywords)} industry keywords\")\n",
    "\n",
    "# 4) GloVe Ï†ÑÏ≤¥ Î°úÎìú\n",
    "glove = {}\n",
    "with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word, vec = parts[0], np.array(parts[1:], dtype=np.float32)\n",
    "        glove[word] = vec\n",
    "logger.info(f\"Loaded {len(glove)} GloVe vectors\")\n",
    "\n",
    "# 5) Íµ¨(phrase) Ï≤òÎ¶¨ Ìï®Ïàò\n",
    "def get_phrase_vector(phrase: str):\n",
    "    toks = phrase.split()\n",
    "    found = [glove[t] for t in toks if t in glove]\n",
    "    if found:\n",
    "        return np.mean(found, axis=0)\n",
    "    # spaCy fallback\n",
    "    docvec = nlp(phrase).vector\n",
    "    return docvec if np.linalg.norm(docvec)>0 else None\n",
    "\n",
    "# 6) ÌÇ§ÏõåÎìúÎ≥Ñ Î≤°ÌÑ∞ ÏàòÏßë\n",
    "words, vecs = [], []\n",
    "for kw in industry_keywords:\n",
    "    if kw in glove:\n",
    "        vec = glove[kw]\n",
    "    else:\n",
    "        vec = get_phrase_vector(kw)\n",
    "    if vec is not None:\n",
    "        words.append(kw)\n",
    "        vecs.append(vec)\n",
    "    else:\n",
    "        logger.warning(f\"No embedding for keyword: {kw}\")\n",
    "\n",
    "if not words:\n",
    "    logger.error(\"No keywords matched any embeddings\")\n",
    "    raise RuntimeError(\"No embeddings to process\")\n",
    "\n",
    "mat = np.vstack(vecs)  # shape [len(words), 300]\n",
    "logger.info(f\"Collected embeddings for {len(words)} keywords\")\n",
    "\n",
    "# 7) PCA Ï∂ïÏÜå (300‚Üí32)\n",
    "pca = PCA(n_components=32, random_state=42)\n",
    "reduced = pca.fit_transform(mat)\n",
    "logger.info(f\"PCA-reduced shape: {reduced.shape}\")\n",
    "\n",
    "# 8) Í≤∞Í≥º Ï†ÄÏû•\n",
    "os.makedirs(os.path.dirname(EMBED_OUT), exist_ok=True)\n",
    "np.save(EMBED_OUT, reduced)\n",
    "with open(VOCAB_OUT, 'w', encoding='utf-8') as f:\n",
    "    json.dump(words, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logger.info(f\"Saved embeddings ‚Üí {EMBED_OUT}\")\n",
    "logger.info(f\"Saved vocab      ‚Üí {VOCAB_OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537aa7b1",
   "metadata": {},
   "source": [
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c9d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_promise_labels.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1) Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "BASE_DIR   = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "DB_PATH    = os.path.join(BASE_DIR, 'data', 'cnn_news.db')\n",
    "VOCAB_PATH = os.path.join(BASE_DIR, 'data', 'industry_vocab.json')\n",
    "OUT_CSV    = os.path.join(BASE_DIR, 'data', 'industry_promise.csv')\n",
    "\n",
    "# 2) Í∏çÏ†ï Îâ¥Ïä§ DB Î°úÎìú\n",
    "engine = create_engine(f'sqlite:///{DB_PATH}')\n",
    "df = pd.read_sql_table('cnn_positive_news', engine)\n",
    "logger.info(f\"Loaded {len(df)} positive news articles from DB\")\n",
    "\n",
    "# 3) ÏÇ∞ÏóÖ ÌÇ§ÏõåÎìú Î™©Î°ù Î°úÎìú\n",
    "with open(VOCAB_PATH, 'r', encoding='utf-8') as f:\n",
    "    industries = json.load(f)\n",
    "logger.info(f\"Loaded {len(industries)} industries from vocab\")\n",
    "\n",
    "# 4) ÌîºÏ≤ò Í≥ÑÏÇ∞\n",
    "records = []\n",
    "for idx, kw in enumerate(industries):\n",
    "    mask    = df['industry_keys'].str.contains(fr'\\b{kw}\\b', case=False, na=False)\n",
    "    df_kw   = df[mask]\n",
    "    freq    = len(df_kw)\n",
    "    # positive_orgs Ïª¨ÎüºÏùÄ comma-separated Ïã¨Î≥º; Í∏∏Ïù¥ > 0 Ïù¥Î©¥ 'Í∏∞ÏóÖ Ïñ∏Í∏â ÎèôÏãú'\n",
    "    pos_rate = (df_kw['positive_orgs'].str.len() > 0).mean() if freq > 0 else 0.0\n",
    "    records.append((idx, freq, pos_rate))\n",
    "\n",
    "logger.info(\"Calculated raw features (freq, pos_rate) for each industry\")\n",
    "\n",
    "# 5) ÌëúÏ§ÄÌôî & Í∞ÄÏ§ëÌï© Î†àÏù¥Î∏î ÏÉùÏÑ±\n",
    "arr    = np.array([[r[1], r[2]] for r in records], dtype=float)\n",
    "scaler = StandardScaler().fit(arr)\n",
    "scaled = scaler.transform(arr)\n",
    "\n",
    "# ÏòàÏãú Í∞ÄÏ§ëÌï©: 0.7*freq_z + 0.3*pos_rate_z\n",
    "weights = np.array([0.7, 0.3])\n",
    "labels  = (scaled * weights).sum(axis=1)\n",
    "\n",
    "logger.info(\"Standardized features and computed promise_label via weighted sum\")\n",
    "\n",
    "# 6) CSVÎ°ú Ï†ÄÏû•\n",
    "out_df = pd.DataFrame({\n",
    "    'industry_id'   : [r[0] for r in records],\n",
    "    'promise_label' : labels,\n",
    "    'feat_freq'     : arr[:, 0],\n",
    "    'feat_pos_rate' : arr[:, 1]\n",
    "})\n",
    "out_df.to_csv(OUT_CSV, index=False)\n",
    "logger.info(f\"Saved promise labels to {OUT_CSV} ({len(out_df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9e28a",
   "metadata": {},
   "source": [
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b859f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_promise_predictor.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1) Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "BASE_DIR       = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "EMBED_PATH     = os.path.join(BASE_DIR, 'data', 'industry_emb_32d.npy')\n",
    "VOCAB_PATH     = os.path.join(BASE_DIR, 'data', 'industry_vocab.json')\n",
    "LABEL_CSV_PATH = os.path.join(BASE_DIR, 'data', 'industry_promise.csv')\n",
    "OUT_MODEL_PATH = os.path.join(BASE_DIR, 'models', 'promise_predictor.pkl')\n",
    "\n",
    "# 2) Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "logger.info(\"Loading embeddings and labels...\")\n",
    "embeddings = np.load(EMBED_PATH)  # shape (N, 32)\n",
    "with open(VOCAB_PATH, 'r', encoding='utf-8') as f:\n",
    "    vocab = json.load(f)          # len(vocab) == embeddings.shape[0]\n",
    "labels_df = pd.read_csv(LABEL_CSV_PATH)\n",
    "\n",
    "# 3) ÌîºÏ≤ò(X)ÏôÄ Î†àÏù¥Î∏î(y) Ï§ÄÎπÑ\n",
    "ids   = labels_df['industry_id'].astype(int).values\n",
    "X_all = embeddings[ids]                 # (M, 32)\n",
    "y_all = labels_df['promise_label'].values  # (M,)\n",
    "\n",
    "logger.info(f\"Prepared X_all shape={X_all.shape}, y_all shape={y_all.shape}\")\n",
    "\n",
    "# 4) ÌïôÏäµ/ÌÖåÏä§Ìä∏ Î∂ÑÌï†\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=42\n",
    ")\n",
    "logger.info(f\"Train/Test split: {len(X_train)}/{len(X_test)}\")\n",
    "\n",
    "# 5) Î™®Îç∏ ÌïôÏäµ (Ridge Regression)\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "logger.info(\"Ridge regression model training complete\")\n",
    "\n",
    "# 6) ÏÑ±Îä• ÌèâÍ∞Ä\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test  = model.predict(X_test)\n",
    "\n",
    "r2_train  = r2_score(y_train, y_pred_train)\n",
    "r2_test   = r2_score(y_test,  y_pred_test)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test  = np.sqrt(mean_squared_error(y_test,  y_pred_test))\n",
    "\n",
    "logger.info(f\"üèÖ Train  R¬≤: {r2_train:.4f}, RMSE: {rmse_train:.4f}\")\n",
    "logger.info(f\"üéØ Test   R¬≤: {r2_test:.4f}, RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# 7) Î™®Îç∏ Ï†ÄÏû•\n",
    "os.makedirs(os.path.dirname(OUT_MODEL_PATH), exist_ok=True)\n",
    "joblib.dump(model, OUT_MODEL_PATH)\n",
    "logger.info(f\"‚úÖ Model saved to {OUT_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1693c747",
   "metadata": {},
   "source": [
    "5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8943f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_contrastive.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1) Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "BASE_DIR             = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "GLOVE_TXT_PATH       = os.path.join(BASE_DIR, 'data', 'glove.6B.300d.txt')\n",
    "GLOVE_PCA_EMBED_PATH = os.path.join(BASE_DIR, 'data', 'glove_pca_64d.npy')\n",
    "GLOVE_PCA_VOCAB_PATH = os.path.join(BASE_DIR, 'data', 'glove_pca_vocab.json')\n",
    "DB_PATH              = os.path.join(BASE_DIR, 'data', 'cnn_news.db')\n",
    "OUT_MODEL_PATH       = os.path.join(BASE_DIR, 'models', 'projector.pth')\n",
    "\n",
    "# 2) GloVe‚ÜíPCA preprocessing if needed\n",
    "if not os.path.exists(GLOVE_PCA_EMBED_PATH):\n",
    "    print(\"üèó Generating Glove-PCA embeddings (64d)‚Ä¶\")\n",
    "    words, vecs = [], []\n",
    "    with open(GLOVE_TXT_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            vals = line.split()\n",
    "            words.append(vals[0])\n",
    "            vecs.append(np.array(vals[1:], dtype=np.float32))\n",
    "    mat = np.vstack(vecs)\n",
    "    pca = PCA(n_components=64)\n",
    "    reduced = pca.fit_transform(mat)\n",
    "    os.makedirs(os.path.dirname(GLOVE_PCA_EMBED_PATH), exist_ok=True)\n",
    "    np.save(GLOVE_PCA_EMBED_PATH, reduced)\n",
    "    with open(GLOVE_PCA_VOCAB_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(words, f, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Saved PCA embeddings ‚Üí {GLOVE_PCA_EMBED_PATH}\")\n",
    "\n",
    "# 3) Load PCA'd GloVe\n",
    "vectors = np.load(GLOVE_PCA_EMBED_PATH)   # shape (V, D)\n",
    "with open(GLOVE_PCA_VOCAB_PATH, 'r', encoding='utf-8') as f:\n",
    "    vocab = json.load(f)                 # length V\n",
    "glove_pca = dict(zip(vocab, vectors))\n",
    "\n",
    "# 4) Load CNN news and vectorize\n",
    "engine = create_engine(f'sqlite:///{DB_PATH}')\n",
    "df = pd.read_sql_table('cnn_positive_news', engine)\n",
    "texts = df['full_text'].dropna().tolist()\n",
    "\n",
    "def get_text_vector(text):\n",
    "    toks = text.split()\n",
    "    vs = [glove_pca[t.lower()] for t in toks if t.lower() in glove_pca]\n",
    "    return np.mean(vs, axis=0) if vs else None\n",
    "\n",
    "# build sample matrix\n",
    "sample_list = [get_text_vector(t) for t in texts]\n",
    "sample_list = [v for v in sample_list if v is not None]\n",
    "samples_np  = np.stack(sample_list)  # shape (N, D)\n",
    "print(f\"üß™ Total contrastive samples: {len(samples_np)}\")\n",
    "\n",
    "# 5) Dataset & DataLoader\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, array: np.ndarray):\n",
    "        # array: NumPy array shape (N, D)\n",
    "        self.vecs = torch.from_numpy(array.astype(np.float32))\n",
    "    def __len__(self):\n",
    "        # leave at least 2 ahead for (pos, neg)\n",
    "        return self.vecs.size(0) - 2\n",
    "    def __getitem__(self, idx):\n",
    "        # return (anchor, positive, negative)\n",
    "        return self.vecs[idx], self.vecs[idx+1], self.vecs[idx+2]\n",
    "\n",
    "loader = DataLoader(ContrastiveDataset(samples_np), batch_size=32, shuffle=True)\n",
    "\n",
    "# 6) Model & optimizer\n",
    "D = samples_np.shape[1]  # embedding dimension (should be 64)\n",
    "class ContrastiveProjector(nn.Module):\n",
    "    def __init__(self, dim=D):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model     = ContrastiveProjector(dim=D).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn   = nn.CosineEmbeddingLoss()\n",
    "\n",
    "# 7) Training loop\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    total_loss = 0.0\n",
    "    for a, p, n in loader:\n",
    "        a, p, n = a.to(device), p.to(device), n.to(device)\n",
    "        za, zp, zn = model(a), model(p), model(n)\n",
    "        y_pos = torch.ones(za.size(0), device=device)\n",
    "        y_neg = -torch.ones(za.size(0), device=device)\n",
    "        loss = loss_fn(za, zp, y_pos) + loss_fn(za, zn, y_neg)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}/{epochs}, Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "# 8) Save projector\n",
    "os.makedirs(os.path.dirname(OUT_MODEL_PATH), exist_ok=True)\n",
    "torch.save(model.state_dict(), OUT_MODEL_PATH)\n",
    "print(f\"‚úÖ Contrastive projector saved ‚Üí {OUT_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565995b7",
   "metadata": {},
   "source": [
    "6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ====== Í≤ΩÎ°ú ÏÑ∏ÌåÖ ======\n",
    "BASE_DIR   = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "DB_PATH    = os.path.join(BASE_DIR, 'data', 'cnn_news.db')\n",
    "VOCAB_PATH = os.path.join(BASE_DIR, 'data', 'industry_vocab.json')\n",
    "OUT_CSV    = os.path.join(BASE_DIR, 'data', 'industry_promise.csv')\n",
    "CUR_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "emb_path = os.path.join(CUR_DIR, 'data', \"industry_emb_32d.npy\")\n",
    "vocab_path = os.path.join(CUR_DIR, 'data', \"industry_vocab.json\")\n",
    "comp_path = os.path.join(CUR_DIR, 'data', \"nasdaq_screener_1744184912302.csv\")\n",
    "model_path = os.path.join(CUR_DIR, 'models',\"promise_predictor.pkl\")\n",
    "output_path = os.path.join(CUR_DIR, 'results', \"company_promise_score.csv\")\n",
    "\n",
    "# ====== ÏûÑÎ≤†Îî© & vocab Î∂àÎü¨Ïò§Í∏∞ ======\n",
    "industry_emb = np.load(emb_path)  # (ÏÇ∞ÏóÖÍ∞úÏàò, 32)\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    industry_vocab = json.load(f)\n",
    "industry2idx = {v.lower(): i for i, v in enumerate(industry_vocab)}\n",
    "\n",
    "# ====== Í∏∞ÏóÖ CSV Î∂àÎü¨Ïò§Í∏∞ ======\n",
    "companies = pd.read_csv(comp_path)\n",
    "# Ïª¨Îüº ÏòàÏãú: Symbol, Name, Industry\n",
    "\n",
    "# ====== Í∏∞ÏóÖ ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ìï®Ïàò ======\n",
    "def extract_industries(industry_str):\n",
    "    if pd.isnull(industry_str):\n",
    "        return []\n",
    "    return [x.strip().lower() for x in industry_str.split(\",\")]\n",
    "\n",
    "def get_company_emb(inds):\n",
    "    idxs = [industry2idx[ind] for ind in inds if ind in industry2idx]\n",
    "    if len(idxs) == 0:\n",
    "        return np.zeros(industry_emb.shape[1])\n",
    "    return np.mean(industry_emb[idxs], axis=0)\n",
    "\n",
    "# ====== Ï†ÑÏ≤¥ Í∏∞ÏóÖ ÏûÑÎ≤†Îî© ÏÉùÏÑ± ======\n",
    "company_emb_list = []\n",
    "for _, row in companies.iterrows():\n",
    "    inds = extract_industries(row['Industry'])\n",
    "    emb = get_company_emb(inds)\n",
    "    company_emb_list.append(emb)\n",
    "company_embs = np.stack(company_emb_list)  # (Í∏∞ÏóÖÏàò, 32)\n",
    "\n",
    "# ====== Ïú†ÎßùÎèÑ ÏòàÏ∏° ======\n",
    "predictor = joblib.load(model_path)\n",
    "scores = predictor.predict(company_embs)\n",
    "companies['promise_score'] = scores\n",
    "\n",
    "# ====== Í≤∞Í≥º Ï†ÄÏû• ======\n",
    "companies.to_csv(output_path, index=False)\n",
    "print(f'>> Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å: {output_path}')\n",
    "print(companies[['Name', 'Industry', 'promise_score']].sort_values('promise_score', ascending=False).head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
