# pipeline/04_extract_keywords_bert.py

import os
import json
from sentence_transformers import SentenceTransformer, util

# ê²½ë¡œ ì„¤ì •
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DATA_DIR = os.path.join(ROOT_DIR, "data")

INPUT_PATH = os.path.join(DATA_DIR, "cnn_keywords_spacy.json")
OUTPUT_PATH = os.path.join(DATA_DIR, "cnn_keywords_bert.json")

# ëª¨ë¸ ë¡œë“œ
print("ğŸ§  Loading Sentence-BERT model...")
model = SentenceTransformer("all-MiniLM-L6-v2")

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
with open(INPUT_PATH, "r") as f:
    articles = json.load(f)

results = []

for article in articles:
    candidates = article.get("keywords", [])
    if not candidates:
        continue

    # ë¬¸ì„œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ stringìœ¼ë¡œ ë§Œë“¤ê¸°
    doc_text = article.get("title", "") + "\n" + article.get("content", "")

    # ì„ë² ë”©
    doc_embedding = model.encode(doc_text, convert_to_tensor=True)
    keyword_embeddings = model.encode(candidates, convert_to_tensor=True)

    # ìœ ì‚¬ë„ ê³„ì‚°
    scores = util.cos_sim(doc_embedding, keyword_embeddings)[0]
    top_indices = scores.topk(k=min(5, len(candidates))).indices.tolist()

    top_keywords = [candidates[i] for i in top_indices]

    results.append({
        "url": article["url"],
        "title": article["title"],
        "date": article["date"],
        "keywords": top_keywords
    })

# ì €ì¥
with open(OUTPUT_PATH, "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"âœ… í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ! ì´ {len(results)}ê°œ ë¬¸ì„œ ì €ì¥ë¨.")
